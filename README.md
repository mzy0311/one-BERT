使用一个BERT来进行机器翻译
对语言模型进行预训练，避免mask、用attention矩阵来控制神经单元的交互，两个句子采用不同的KQV向量，预训练是否为翻译关系。
